{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/covillarreal/AprendizajeAutomatico/blob/main/tp4_chatbot_retrieval_template__villarreal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hghx5BwKdm-A"
      },
      "outputs": [],
      "source": [
        "!pip install spacy --quiet\n",
        "!python -m spacy download es_core_news_sm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "doc = nlp(\"Esto es una frase.\")\n",
        "print([(w.text, w.pos_) for w in doc])"
      ],
      "metadata": {
        "id": "D9SRh-eDdpWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbots basados en recuperación\n",
        "\n",
        "En inglés information retrieval chatbots"
      ],
      "metadata": {
        "id": "UnrG-1ylkncZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motor de búsqueda"
      ],
      "metadata": {
        "id": "s3bs1FmWkfrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Búsqueda por palabras clave: Extrae palabras clave de la pregunta del usuario y busca coincidencias en las preguntas almacenadas.\n",
        "\n",
        "* Similitud del coseno: Si has representado las preguntas como vectores (por ejemplo, usando TF-IDF o word embeddings), puedes usar la similitud del coseno para medir la distancia entre las preguntas.\n",
        "\n",
        "* Word embeddings: Utiliza modelos de word embeddings como Word2Vec o BERT para obtener representaciones semánticas de las preguntas y las consultas del usuario."
      ],
      "metadata": {
        "id": "enbz9kXGkWlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Búsqueda por palabras claves"
      ],
      "metadata": {
        "id": "UQoVRp4gjxUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tu_diccionario = {\n",
        "   \"hola\": \"¡Hola! ¿En qué puedo ayudarte?\",\n",
        "   \"adiós\": \"Hasta luego. ¡Que tengas un buen día!\",\n",
        "   \"información\": \"¿Qué tipo de información estás buscando?\",\n",
        "   # Agrega más entradas de diccionario según tus necesidades\n",
        "}\n"
      ],
      "metadata": {
        "id": "3BljEMEOhpTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_pregunta(pregunta):\n",
        "    pregunta_procesada = nlp(pregunta.lower())  # Procesa la pregunta y convierte a minúsculas\n",
        "    respuesta = \"Lo siento, no entiendo tu pregunta.\"\n",
        "\n",
        "    # Busca una coincidencia en el diccionario\n",
        "    for palabra in pregunta_procesada:\n",
        "        # regresa la primer coincidencia que encuentra\n",
        "        if palabra.text in tu_diccionario:\n",
        "            respuesta = tu_diccionario[palabra.text]\n",
        "            break\n",
        "\n",
        "    return respuesta\n"
      ],
      "metadata": {
        "id": "Z4q_k1_3hvyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    entrada_usuario = input(\"Tú: \")\n",
        "    if entrada_usuario.lower() == \"salir\":\n",
        "        print(\"Chatbot: Hasta luego.\")\n",
        "        break\n",
        "    respuesta = responder_pregunta(entrada_usuario)\n",
        "    print(\"Chatbot:\", respuesta)\n"
      ],
      "metadata": {
        "id": "9gMPKi-ghwcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6S88twrY5sOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Búsqueda por similitud"
      ],
      "metadata": {
        "id": "nI2FsyUOj3je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para los chatbots basados ​​en recuperación, es común utilizar bolsas de palabras (bag of words) o tf-idf para calcular la similitud de intenciones."
      ],
      "metadata": {
        "id": "AoZIaX0Kj7xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Datos de ejemplo\n",
        "preguntas = [\"¿Qué es el aprendizaje automático?\",\n",
        "             \"¿Cómo funciona la regresión lineal?\"]\n",
        "respuestas = [\"El aprendizaje automático es una rama de la inteligencia artificial...\",\n",
        "              \"La regresión lineal es un método de modelado...\"]\n",
        "\n",
        "# Vectorización con TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preguntas)\n",
        "\n",
        "# Función para encontrar la mejor coincidencia\n",
        "def responder_pregunta(consulta_usuario):\n",
        "    consulta_vec = vectorizer.transform([consulta_usuario])\n",
        "    similitudes = cosine_similarity(consulta_vec, tfidf_matrix).flatten()\n",
        "    print(similitudes)\n",
        "    indice_mejor_coincidencia = similitudes.argmax()\n",
        "    print(indice_mejor_coincidencia)\n",
        "    return respuestas[indice_mejor_coincidencia]\n"
      ],
      "metadata": {
        "id": "CReIz0ISj75s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ejemplo de consulta\n",
        "consulta = \"¿Qué es la regresión lineal?\"\n",
        "print(responder_pregunta(consulta))\n"
      ],
      "metadata": {
        "id": "foqaZ1FN583i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Búsqueda por similitud en embeddings"
      ],
      "metadata": {
        "id": "20KCxfCymOHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedes vectorizar el texto usando embeddings, como vimos la clase pasada.\n"
      ],
      "metadata": {
        "id": "7g6jgLSLnilX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividades\n",
        "\n",
        "### 1) Elaborar un dataset de preguntas y respuestas para crear un Chatbot para un aplicación particular. ( 3 puntos )\n",
        "\n",
        "  1.1 Debe definir la aplicación (atención al cliente bancario, atención a estudiantes universitarios, etc).\n",
        "\n",
        "  1.2 El listado de preguntas y respuestas debe tener como mínimo 20 elementos pregunta - respuesta.\n",
        "\n",
        "###  2) Crear el chatbot utilizando TFIDF y similitud del coseno. (1 punto)\n",
        "\n",
        "### 3) Crear otro chatbot utilizando embeddings. Indique cuál embedding (1 punto) pre-entrenado eligió.\n",
        "\n",
        "### 4) Muestra ambos chatbots funcionando (1 punto)\n",
        "\n",
        "Adjuntar la lista de preguntas utilizadas para probar el funcionamiento.\n",
        "\n",
        "### 5) Añade tus conclusiones de todo lo realizado (2 punto)\n",
        "\n",
        "### 6) BONUS: usa lo realizado en 1 y 3 para crear un chatbot RAG. (2 puntos)\n",
        "\n",
        "* Utiliza un modelo LLM pre-entrenado.\n",
        "\n",
        "* Este punto no es obligatorio de realizar para quienes quieran regularizar / recuperar y luego rendirán en mesa.\n",
        "* Para quienes tienen condiciones para promocionar (han realizado y entregado los TPs a tiempo) la resolución de este ejercicio será tenida en cuenta para sumar a la promoción.\n",
        "\n",
        "### 7) No olvides:\n",
        "\n",
        "* Explicar tus decisiones y configuraciones. Añadir tus conclusiones.\n",
        "* Anunciar en el foro cuál será tu aplicación y postear tu entrega y tus avances.\n",
        "* Debes subir tu notebook a un repo GitHub público de tu propiedad compartido + enlace colab.\n",
        "* Documentar todo el proceso.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1sGxF1VglJVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APLICACIÓN:** Mi Chatbot será aplicado en la atención al cliente de una aerolinea argentina."
      ],
      "metadata": {
        "id": "6g982_ztFttF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preguntas_aerolinea = [\n",
        "    \"¿Cómo puedo reservar un vuelo?\",\n",
        "    \"¿Cuáles son los métodos de pago?\",\n",
        "    \"¿Puedo cancelar mi reserva y obtener un reembolso?\",\n",
        "    \"¿Cuántas maletas puedo llevar en un vuelo nacional?\",\n",
        "    \"¿Cuáles son las medidas permitidas para el equipaje de mano?\",\n",
        "    \"¿Cómo puedo llevar equipaje extra?\",\n",
        "    \"¿Cuándo puedo hacer el check-in online?\",\n",
        "    \"¿Necesito imprimir mi tarjeta de embarque?\",\n",
        "    \"¿Hasta qué hora puedo llegar al aeropuerto para tomar mi vuelo?\",\n",
        "    \"¿Cómo cambio la fecha de mi vuelo?\",\n",
        "    \"¿Qué pasa si pierdo mi vuelo?\",\n",
        "    \"¿Cómo solicito asistencia especial en el aeropuerto?\",\n",
        "    \"¿Tienen descuentos para estudiantes o jubilados?\",\n",
        "    \"¿Cómo acumulo millas con mi vuelo?\",\n",
        "    \"¿Cuándo es el mejor momento para conseguir vuelos baratos?\",\n",
        "    \"¿Se puede abonar en cuotas?\",\n",
        "    \"¿Puedo llevar un animal abordo?\",\n",
        "    \"¿A partir de que edad se debe abonar una tarifa?\",\n",
        "    \"¿Puedo comprar un vuelo con fechas flexibles?\",\n",
        "    \"¿Puedo modificar los datos de mi reserva?\",\n",
        "    \"¿Puedo transferir mi pasaje a otra persona?\"\n",
        "]\n",
        "\n",
        "respuestas_aerolinea = [\n",
        "    \"Puedes reservar un vuelo en nuestra web o llamando a atención al cliente.\",\n",
        "    \"Aceptamos tarjetas de crédito, débito, transferencias y pagos en agencias autorizadas.\",\n",
        "    \"Depende de la tarifa adquirida. Algunas permiten reembolsos y otras solo cambios de fecha.\",\n",
        "    \"En vuelos nacionales puedes llevar una pieza de equipaje de mano y una maleta facturada, según la tarifa. Tambien puedes agregar maletas con una tarifa adicional\",\n",
        "    \"El equipaje de mano debe tener un máximo de 55 x 35 x 25 cm y no superar los 9 kg.\",\n",
        "    \"Puedes comprar equipaje adicional durante la reserva o gestionarlo en el aeropuerto. En el aeropuerto tiene un costo mas elevado\",\n",
        "    \"El check-in online está disponible desde 48 hasta 4 horas antes del vuelo.\",\n",
        "    \"No es obligatorio, puedes mostrarlo en tu celular.\",\n",
        "    \"Se recomienda estar 2 horas antes para vuelos nacionales y 3 horas antes para internacionales.\",\n",
        "    \"Puedes modificar tu vuelo en nuestra web o llamando a atención al cliente.\",\n",
        "    \"Debes comunicarte con la aerolínea lo antes posible para conocer opciones de reprogramación.\",\n",
        "    \"Puedes solicitar asistencia para personas con movilidad reducida o necesidades especiales al reservar el vuelo.\",\n",
        "    \"Sí, ofrecemos tarifas especiales para estudiantes y jubilados en ciertos vuelos.\",\n",
        "    \"Si eres miembro de nuestro programa de viajero frecuente, acumulas millas automáticamente al comprar vuelos.\",\n",
        "    \"Generalmente, comprando con anticipación y aprovechando nuestras promociones de temporada.\",\n",
        "    \"Puedes abonar en cuotas sin ningún problema. Depende de tu banco la financiacion que tengas disponible.\",\n",
        "    \"Dentro de nuestra página se encuentran los requisitos para viajar con animales abordo. Ten en cuenta que tiene una tarifa.\",\n",
        "    \"A partir de los 3 años se abona la tarifa normal\",\n",
        "    \"Si, hay tarifas con flechas flexibles. Al momento de comprar tu pasaje te ofrece este tipo de tarifa.\",\n",
        "    \"Si, ingresando a tu reserva puedes realizar los cambios de datos personales.\",\n",
        "    \"No, lo lamento, aún no podemos autorizar la transferencia de pasajes.\"\n",
        "]"
      ],
      "metadata": {
        "id": "7Byx3yeqG7hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Vectorización con TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preguntas_aerolinea)\n",
        "\n",
        "# Función para encontrar la mejor coincidencia\n",
        "def responder_pregunta_tfidf(consulta_usuario):\n",
        "    consulta_vec = vectorizer.transform([consulta_usuario]) # Vectorizo la consulta\n",
        "    similitudes = cosine_similarity(consulta_vec, tfidf_matrix).flatten() # Calculo la similitud con cada pregunta\n",
        "    indice_mejor_coincidencia = similitudes.argmax() # Obtengo el índice de la mejor coincidencia\n",
        "    return respuestas_aerolinea[indice_mejor_coincidencia] # Devuelvo la respuesta correcta\n"
      ],
      "metadata": {
        "id": "PpNEQvNHjco9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este código, creé un chatbot para **servicio al cliente de una aerolínea** usando TF-IDF y similitud del coseno.\n",
        "\n",
        "Primero, definí un conjunto de preguntas y respuestas que tratan de temas como reservas, pagos, equipaje, check-in y promociones. Así, el chatbot puede responder a las consultas más frecuentes de los pasajeros.\n",
        "\n",
        "Luego, vectoricé las preguntas con TF-IDF, lo que significa que convertí las frases en números para que el sistema pueda compararlas matemáticamente. Al hacer esto, cada pregunta tiene un formato que permite medir su similitud con otras consultas.\n",
        "\n",
        "Después, creé una función para responder preguntas. Esta función toma la consulta del usuario, la transforma en un vector, calcula su similitud con las preguntas del dataset y elige la respuesta más cercana. Para ello, uso *cosine_similarity* y *argmax*, que identifica la mejor coincidencia.\n",
        "\n",
        "Finalmente, cuando el usuario hace una pregunta, el chatbot encuentra la respuesta más relevante según la similitud del texto y la muestra en pantalla.\n",
        "\n",
        "Así logré que el chatbot entienda consultas similares a las que tiene en su dataset y responda.\n"
      ],
      "metadata": {
        "id": "-WNQk6tlijRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# cargo el modelo para generar embeddings en español (all-MiniLM-L6-v2)\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# genero los embeddings para cada pregunta del dataset\n",
        "preguntas_embeddings = model.encode(preguntas_aerolinea)\n",
        "\n",
        "def responder_pregunta_embedding(consulta_usuario):\n",
        "    \"\"\"\n",
        "    Dada la consulta del usuario, genera su embedding y calcula la similitud\n",
        "    con los embeddings de mis preguntas. Devuelve la respuesta asociada\n",
        "    a la pregunta más similar.\n",
        "    \"\"\"\n",
        "    consulta_embedding = model.encode([consulta_usuario])[0]\n",
        "    similitudes = cosine_similarity([consulta_embedding], preguntas_embeddings).flatten()\n",
        "    indice_mejor = np.argmax(similitudes)\n",
        "    return respuestas_aerolinea[indice_mejor]"
      ],
      "metadata": {
        "id": "UVVlqsRi4pni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este punto, lo que hago es usar el modelo all-MiniLM-L6-v2 de Sentence Transformers para transformar las preguntas en representaciones numéricas (embeddings) que capturan su significado. Primero, cargo el modelo, lo que me permite convertir cada una de mis preguntas en un vector. Luego, convierto todo mi conjunto de preguntas del dataset en estos embeddings, lo que me facilita compararlas con nuevas consultas. Cada vez que un usuario me hace una pregunta, genero el embedding de esa consulta y calculo qué tan similar es a los embeddings de mis preguntas ya conocidas, utilizando la similitud del coseno.\n",
        "\n",
        "La pregunta con la mayor similitud me indica cuál es la respuesta más apropiada, y devuelvo la respuesta asociada a esa pregunta. Es una manera intuitiva de reconocer la intención del usuario basándome en el significado de las oraciones, en lugar de solo en las palabras que las componen.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2aE7i4EVptQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lista de preguntas usadas para probar\n",
        "preguntas_prueba = [\n",
        "    \"¿Cómo puedo modificar mi vuelo?\",\n",
        "    \"¿Cuándo puedo hacer el check-in?\",\n",
        "    \"¿Qué pasa si pierdo mi vuelo?\",\n",
        "    \"¿Puedo abonar en cuotas?\",\n",
        "    \"¿Cómo acumulo millas?\",\n",
        "]\n",
        "\n",
        "# Pruebas del chatbot con TF-IDF\n",
        "print(\"Resultados Chatbot TF-IDF:\")\n",
        "for pregunta in preguntas_prueba:\n",
        "    print(\"Usuario:\", pregunta)\n",
        "    print(\"Chatbot:\", responder_pregunta_tfidf(pregunta))\n",
        "    print()\n",
        "\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "# Pruebas del chatbot con embeddings (all-MiniLM-L6-v2)\n",
        "print(\"Resultados Chatbot con Embeddings (all-MiniLM-L6-v2):\")\n",
        "for pregunta in preguntas_prueba:\n",
        "    print(\"Usuario:\", pregunta)\n",
        "    print(\"Chatbot:\", responder_pregunta_embedding(pregunta))\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "JKzt5xtfwhPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este código, preparé una lista de preguntas de prueba y luego usé cada uno de mis chatbots para responderlas. Primero, ejecuté el chatbot basado en TF-IDF, imprimiendo cada pregunta y la respuesta correspondiente. Después, hice lo mismo con el chatbot que usa embeddings (all-MiniLM-L6-v2). De esta manera, pude comparar fácilmente las respuestas de ambos enfoques.\n",
        "\n",
        "Pude observar que el chatbot con TF-IDF devolvió una respuesta que se centra en modificar datos de la reserva, mientras que el enfoque con embeddings apuntó a reservar el vuelo, mostrando una diferencia en la selección de la respuesta para esa consulta. Las demás respuestas fueron casi iguales, lo que demuestra que ambos métodos en general reconocen bien el sentido de la pregunta."
      ],
      "metadata": {
        "id": "q9UDjTmhy2wZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONCLUSIÓN\n",
        "En este trabajo trabajé con dos enfoques para crear un chatbot de atención al cliente: uno basado en TF-IDF y otro utilizando embeddings con el modelo pre-entrenado all-MiniLM-L6-v2. Noté que, mientras el método TF-IDF ofrece respuestas coherentes en todos los casos que se vieron de ejemplo, el enfoque basado en embeddings es tambien excepto en un caso que selecciona una respuesta distinta. Para mi es importante seguir probando con otros modelos hasta encontrar alguno con mejores resultados.\n",
        "\n",
        "Este ejercicio me permitió aprender y comparar dos métodos diferentes, dándome cuenta de que aún tengo mucho por investigar, aprender y, sobre todo, practicar en el campo del procesamiento del lenguaje natural.\n",
        "\n"
      ],
      "metadata": {
        "id": "A_AAxQhWyWTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importo las librerías necesarias\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1) Embeddings con all-MiniLM-L6-v2 (igual que el punto 3)\n",
        "modelo_embeddings = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Dataset de preguntas y respuestas del punto 1\n",
        "preguntas = [\n",
        "    \"¿Cómo puedo reservar un vuelo?\",\n",
        "    \"¿Cuáles son los métodos de pago?\",\n",
        "    \"¿Puedo cancelar mi reserva y obtener un reembolso?\",\n",
        "    \"¿Cuántas maletas puedo llevar en un vuelo nacional?\",\n",
        "    \"¿Cuáles son las medidas permitidas para el equipaje de mano?\",\n",
        "    \"¿Cómo puedo llevar equipaje extra?\",\n",
        "    \"¿Cuándo puedo hacer el check-in online?\",\n",
        "    \"¿Necesito imprimir mi tarjeta de embarque?\",\n",
        "    \"¿Hasta qué hora puedo llegar al aeropuerto para tomar mi vuelo?\",\n",
        "    \"¿Cómo cambio la fecha de mi vuelo?\",\n",
        "    \"¿Qué pasa si pierdo mi vuelo?\",\n",
        "    \"¿Cómo solicito asistencia especial en el aeropuerto?\",\n",
        "    \"¿Tienen descuentos para estudiantes o jubilados?\",\n",
        "    \"¿Cómo acumulo millas con mi vuelo?\",\n",
        "    \"¿Cuándo es el mejor momento para conseguir vuelos baratos?\",\n",
        "    \"¿Se puede abonar en cuotas?\",\n",
        "    \"¿Puedo llevar un animal abordo?\",\n",
        "    \"¿A partir de qué edad se debe abonar una tarifa?\",\n",
        "    \"¿Puedo comprar un vuelo con fechas flexibles?\",\n",
        "    \"¿Puedo modificar los datos de mi reserva?\",\n",
        "    \"¿Puedo transferir mi pasaje a otra persona?\"\n",
        "]\n",
        "\n",
        "respuestas = [\n",
        "    \"Puedes reservar un vuelo en nuestra web, aplicación móvil o llamando a atención al cliente.\",\n",
        "    \"Aceptamos tarjetas de crédito, débito, transferencias y pagos en agencias autorizadas.\",\n",
        "    \"Depende de la tarifa adquirida. Algunas permiten reembolsos y otras solo cambios de fecha.\",\n",
        "    \"En vuelos nacionales puedes llevar una pieza de equipaje de mano y una maleta facturada, según la tarifa.\",\n",
        "    \"El equipaje de mano debe tener un máximo de 55 x 35 x 25 cm y no superar los 9 kg.\",\n",
        "    \"Puedes comprar equipaje adicional durante la reserva o gestionarlo en el aeropuerto. En el aeropuerto tiene un costo mayor.\",\n",
        "    \"El check-in online está disponible desde 48 hasta 4 horas antes del vuelo.\",\n",
        "    \"No es obligatorio, puedes mostrarla en tu celular.\",\n",
        "    \"Se recomienda estar 2 horas antes para vuelos nacionales y 3 para internacionales.\",\n",
        "    \"Puedes modificar tu vuelo en nuestra web o llamando a atención al cliente.\",\n",
        "    \"Debes comunicarte con la aerolínea lo antes posible para conocer opciones de reprogramación.\",\n",
        "    \"Puedes solicitar asistencia especial al momento de reservar o al llegar al aeropuerto.\",\n",
        "    \"Sí, ofrecemos tarifas especiales para estudiantes y jubilados en ciertos vuelos.\",\n",
        "    \"Si eres miembro de nuestro programa de viajeros frecuentes, acumulas millas automáticamente.\",\n",
        "    \"Generalmente, los vuelos más económicos se consiguen comprando con anticipación y aprovechando promociones.\",\n",
        "    \"Puedes abonar en cuotas, dependiendo de tu banco.\",\n",
        "    \"Consulta los requisitos para viajar con animales, ya que tiene una tarifa adicional.\",\n",
        "    \"A partir de 3 años se abona la tarifa normal.\",\n",
        "    \"Sí, existen tarifas flexibles, las cuales se muestran durante la compra.\",\n",
        "    \"Puedes modificar los datos de tu reserva directamente en nuestra web.\",\n",
        "    \"Por el momento, la transferencia de pasajes no está habilitada.\"\n",
        "]\n",
        "\n",
        "preguntas_embeddings = modelo_embeddings.encode(preguntas)\n",
        "\n",
        "# 4) Defino una función para recuperar el contexto (en este caso, la respuesta) basado en la consulta del usuario.\n",
        "def recuperar_contexto(pregunta_usuario):\n",
        "    # Transformo en embedding la pregunta del usuario.\n",
        "    emb_usuario = modelo_embeddings.encode([pregunta_usuario])[0]\n",
        "    # Calculo la similitud del coseno entre la consulta del usuario y cada pregunta del dataset.\n",
        "    sims = cosine_similarity([emb_usuario], preguntas_embeddings).flatten()\n",
        "    # Selecciono el índice de la pregunta con mayor similitud.\n",
        "    idx = np.argmax(sims)\n",
        "    return respuestas[idx], sims[idx]\n",
        "\n",
        "# 5) Configuro el generador de respuestas usando un LLM pre entrenado.\n",
        "# Aquí uso \"google/flan-t5-small\" en modo text2text-generation para generar respuestas a partir de un prompt.\n",
        "generador = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-small\",\n",
        "    tokenizer=\"google/flan-t5-small\",\n",
        "    device=0  # Uso GPU si está disponible; si no, se usará CPU.\n",
        ")\n",
        "\n",
        "# 6) Defino la función que combina ambas etapas (retrieval y generation) para dar una respuesta final.\n",
        "def responder_RAG(pregunta_usuario):\n",
        "    # Primero, recupero el contexto más relevante del dataset.\n",
        "    contexto, score = recuperar_contexto(pregunta_usuario)\n",
        "\n",
        "    # Luego, creo un prompt que combine la información recuperada con la pregunta del usuario.\n",
        "    # Es importante indicar claramente que el agente debe responder en español y de forma breve.\n",
        "    prompt = (\n",
        "        \"Eres un agente de atención al cliente de una aerolínea en Argentina.\\n\"\n",
        "        \"Responde en español y de manera breve y útil.\\n\\n\"\n",
        "        f\"Contexto: {contexto}\\n\"\n",
        "        f\"Pregunta: {pregunta_usuario}\\n\"\n",
        "        \"Respuesta: \"\n",
        "    )\n",
        "\n",
        "    # Genero la respuesta utilizando el modelo generativo.\n",
        "    # Aquí controlo la longitud de la respuesta usando max_new_tokens.\n",
        "    salida = generador(\n",
        "        prompt,\n",
        "        max_new_tokens=50,\n",
        "        num_beams=4,        # Uso beam search para obtener una respuesta de calidad.\n",
        "        early_stopping=True\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    # Extraigo la parte de la respuesta que viene luego del prompt.\n",
        "    respuesta_final = salida.replace(prompt, \"\").strip()\n",
        "    return respuesta_final\n",
        "\n",
        "# 7) Realizo pruebas con algunas preguntas de ejemplo.\n",
        "preguntas_prueba = [\n",
        "    \"¿Cómo puedo modificar mi vuelo?\",\n",
        "    \"¿Cuándo puedo hacer el check-in?\",\n",
        "    \"¿Qué pasa si pierdo mi vuelo?\",\n",
        "    \"¿Puedo abonar en cuotas?\",\n",
        "    \"¿Cómo acumulo millas?\"\n",
        "]\n",
        "\n",
        "print(\"=== Chatbot RAG (Retrieval-Augmented Generation) ===\")\n",
        "for p in preguntas_prueba:\n",
        "    print(\"Usuario:\", p)\n",
        "    respuesta = responder_RAG(p)\n",
        "    print(\"Chatbot:\", respuesta)\n",
        "    print()"
      ],
      "metadata": {
        "id": "FDcV_NME11qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Carga y preparación del dataset:\n",
        "Importo un modelo de embeddings (all‑MiniLM‑L6‑v2) y defino una lista de preguntas con sus respuestas (Igual al punto 1). Luego, convierto cada pregunta en un vector para poder compararlas fácilmente.\n",
        "- Recuperación del contexto:\n",
        "Con la función recuperar_contexto, transformo la consulta del usuario en un vector y calculo la similitud con los vectores de mi dataset. Así, elijo la respuesta que tenga el mayor parecido como el contexto relevante.\n",
        "- Generación de la respuesta final (RAG):\n",
        "Utilizo un modelo LLM pre-entrenado (google/flan-t5-small) para combinar el contexto recuperado y la pregunta del usuario en un prompt. El LLM genera una respuesta breve y útil, que luego muestro al usuario.\n",
        "\n",
        "**Conclusión:**\n",
        "\n",
        "He logrado crear un chatbot RAG que primero identifica la respuesta más relevante de mi base de conocimiento y luego la mejora con un modelo generativo para dar una respuesta natural. Creo que el resultado es no es muy bueno, pero con más tiempo exploraría ajustar el prompt y probar modelos de generación más robustos para obtener respuestas aún más precisas y adaptadas al usuario. La verdad que este trabajo me demanda mucho mas tiempo del que me hubiera gustado disponer para obtener un mejor resultado o intentar distintas maneras y luego comparar.\n"
      ],
      "metadata": {
        "id": "clJmU_1LG4KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RECURSOS:\n",
        "- [Página: Hugging Face](https://huggingface.co/docs/transformers/model_doc/rag)\n",
        "- Copilot IA\n",
        "- Clases de la materia\n",
        "\n"
      ],
      "metadata": {
        "id": "bzV7LyzRrSrG"
      }
    }
  ]
}